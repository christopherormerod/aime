
@article{piresSabiaPortugueseLarge2023,
	title = {Sabiá: {Portuguese} {Large} {Language} {Models}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Sabiá},
	url = {https://arxiv.org/abs/2304.07880},
	doi = {10.48550/ARXIV.2304.07880},
	abstract = {As the capabilities of language models continue to advance, it is conceivable that "one-size-fits-all" model will remain as the main paradigm. For instance, given the vast number of languages worldwide, many of which are low-resource, the prevalent practice is to pretrain a single model on multiple languages. In this paper, we add to the growing body of evidence that challenges this practice, demonstrating that monolingual pretraining on the target language significantly improves models already extensively trained on diverse corpora. More specifically, we further pretrain GPT-J and LLaMA models on Portuguese texts using 3\% or less of their original pretraining budget. Few-shot evaluations on Poeta, a suite of 14 Portuguese datasets, reveal that our models outperform English-centric and multilingual counterparts by a significant margin. Our best model, Sabiá-65B, performs on par with GPT-3.5-turbo. By evaluating on datasets originally conceived in the target language as well as translated ones, we study the contributions of language-specific pretraining in terms of 1) capturing linguistic nuances and structures inherent to the target language, and 2) enriching the model's knowledge about a domain or culture. Our results indicate that the majority of the benefits stem from the domain-specific knowledge acquired through monolingual pretraining.},
	urldate = {2023-04-20},
	author = {Pires, Ramon and Abonizio, Hugo and Almeida, Thales Sales and Nogueira, Rodrigo},
	year = {2023},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), fine-tune, FOS: Computer and information sciences, LLM},
}

@article{xiaoConversationalAgentsLanguage2023,
	title = {Conversational agents in language learning},
	issn = {2748-3479},
	url = {https://www.degruyter.com/document/doi/10.1515/jccall-2022-0032/html},
	doi = {10.1515/jccall-2022-0032},
	abstract = {Due to advances in technology, conversational agents are emerging as intelligent spoken dialogue systems that simulate natural conversation with human beings. A growing body of literature has investigated the potential of conversational agents in enhancing language learning across multiple contexts. In this paper, a broad scoping review examining the current literature on conversational agents and language learning was conducted. This review mapped APA PsycINFO, ERIC and ProQuest Dissertations \&amp; Theses databases, which yielded 23 papers for further analysis. Our examination of these papers suggests that there are three main ways in which conversational agents are used for language learning. This review discusses these three approaches and points to directions that require further research to fully exploit the potential of conversational agents in language learning.},
	language = {en},
	urldate = {2023-03-29},
	journal = {Journal of China Computer-Assisted Language Learning},
	author = {Xiao, Feiwen and Zhao, Priscilla and Sha, Hanyue and Yang, Dandan and Warschauer, Mark},
	month = mar,
	year = {2023},
	note = {Publisher: De Gruyter},
	keywords = {technology, CALL, conversational agents, language learning, natural language processing (NLP)},
	file = {Full Text PDF:/Users/johnsmachine/Zotero/storage/FPSJEQXD/Xiao et al. - 2023 - Conversational agents in language learning.pdf:application/pdf},
}

@misc{eloundouGPTsAreGPTs2023,
	title = {{GPTs} are {GPTs}: {An} {Early} {Look} at the {Labor} {Market} {Impact} {Potential} of {Large} {Language} {Models}},
	shorttitle = {{GPTs} are {GPTs}},
	url = {http://arxiv.org/abs/2303.10130},
	abstract = {We investigate the potential implications of Generative Pre-trained Transformer (GPT) models and related technologies on the U.S. labor market. Using a new rubric, we assess occupations based on their correspondence with GPT capabilities, incorporating both human expertise and classifications from GPT-4. Our findings indicate that approximately 80\% of the U.S. workforce could have at least 10\% of their work tasks affected by the introduction of GPTs, while around 19\% of workers may see at least 50\% of their tasks impacted. The influence spans all wage levels, with higher-income jobs potentially facing greater exposure. Notably, the impact is not limited to industries with higher recent productivity growth. We conclude that Generative Pre-trained Transformers exhibit characteristics of general-purpose technologies (GPTs), suggesting that these models could have notable economic, social, and policy implications.},
	urldate = {2023-03-25},
	publisher = {arXiv},
	author = {Eloundou, Tyna and Manning, Sam and Mishkin, Pamela and Rock, Daniel},
	month = mar,
	year = {2023},
	note = {arXiv:2303.10130 [cs, econ, q-fin]},
	keywords = {Computer Science - Computers and Society, Computer Science - Artificial Intelligence, Economics - General Economics},
	file = {arXiv Fulltext PDF:/Users/johnsmachine/Zotero/storage/DM2IHWLY/Eloundou et al. - 2023 - GPTs are GPTs An Early Look at the Labor Market I.pdf:application/pdf;arXiv.org Snapshot:/Users/johnsmachine/Zotero/storage/GXLFFHCP/2303.html:text/html},
}

@misc{tateEducationalResearchAIGenerated2023,
	title = {Educational {Research} and {AI}-{Generated} {Writing}: {Confronting} the {Coming} {Tsunami}},
	shorttitle = {Educational {Research} and {AI}-{Generated} {Writing}},
	url = {https://edarxiv.org/4mec3/},
	doi = {10.35542/osf.io/4mec3},
	abstract = {The public release and surprising capacity of ChatGPT has brought AI-enabled text generation into the forefront for educators and academics. ChatGPT and similar text generation tools raise numerous questions for educational practitioners, policymakers, and researchers. We begin by first describing what large language models are and how they function, and then situate them in the history of technology’s complex interrelationship with literacy, cognition, and education. Finally, we discuss implications for the field.},
	language = {en-us},
	urldate = {2023-03-17},
	publisher = {EdArXiv},
	author = {Tate, Tamara and Doroudi, Shayan and Ritchie, Daniel and Xu, Ying and Uci, Mark Warschauer},
	month = jan,
	year = {2023},
	keywords = {Education, technology, education, Artificial Intelligence, ml, Language and Literacy Education, literacy},
	file = {Full Text PDF:/Users/johnsmachine/Zotero/storage/6J846SWG/Tate et al. - 2023 - Educational Research and AI-Generated Writing Con.pdf:application/pdf},
}

@inproceedings{zhengMxMLExploringParadigmatic2023,
	address = {Chicago, IL},
	title = {{MxML} ({Exploring} the paradigmatic relationship between measurement and machine learning in the history, current time, and future): {Current} state-of-the-field},
	doi = {https://edarxiv.org/n9reh},
	abstract = {The recent surge of machine learning (ML) has impacted many disciplines, including educational and psychological measurement (hereafter shortened as measurement, “M”). The
measurement literature has seen a rapid growth in studies that explore using ML methods to solve measurement problems. However, there exist gaps between the typical paradigm of ML and fundamental principles of measurement. The MxML project was created to explore how the measurement community might potentially redefine the psychometrics discipline in the imminent
future of big data and machine learning, so as to harness the power of machine learning to serve
our (redefined and updated) mission. This paper describes the first study of the MxML project, in
which we summarize the state of the field of applications, extensions, and discussions about ML methods in measurement contexts with a systematic review of the recent 10 years of literature
(2013 - 2022). Specifically, we provide a snapshot of the literature in terms of (1) areas of measurement, (2) types of article, (3) ML methods discussed, and (4) gaps addressed between measurement goals and ML methods.},
	author = {Zheng, Yi and Nydick, Steven and Huang, Sijia and Zhang, Susu},
	month = apr,
	year = {2023},
	file = {Zheng et al. - 2023 - MxML (Exploring the paradigmatic relationship betw.pdf:/Users/johnsmachine/Zotero/storage/6Y92FCUU/Zheng et al. - 2023 - MxML (Exploring the paradigmatic relationship betw.pdf:application/pdf},
}

@misc{elkinsHowUsefulAre2023,
	title = {How {Useful} are {Educational} {Questions} {Generated} by {Large} {Language} {Models}?},
	url = {http://arxiv.org/abs/2304.06638},
	abstract = {Controllable text generation (CTG) by large language models has a huge potential to transform education for teachers and students alike. Specifically, high quality and diverse question generation can dramatically reduce the load on teachers and improve the quality of their educational content. Recent work in this domain has made progress with generation, but fails to show that real teachers judge the generated questions as sufficiently useful for the classroom setting; or if instead the questions have errors and/or pedagogically unhelpful content. We conduct a human evaluation with teachers to assess the quality and usefulness of outputs from combining CTG and question taxonomies (Bloom's and a difficulty taxonomy). The results demonstrate that the questions generated are high quality and sufficiently useful, showing their promise for widespread use in the classroom setting.},
	urldate = {2023-04-21},
	publisher = {arXiv},
	author = {Elkins, Sabina and Kochmar, Ekaterina and Cheung, Jackie C. K. and Serban, Iulian},
	month = apr,
	year = {2023},
	note = {arXiv:2304.06638 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/johnsmachine/Zotero/storage/FHDEMGTG/Elkins et al. - 2023 - How Useful are Educational Questions Generated by .pdf:application/pdf;arXiv.org Snapshot:/Users/johnsmachine/Zotero/storage/ABC4X8NU/2304.html:text/html},
}

@misc{ThingsEverySchool2023,
	title = {5 {Things} {Every} {School} {District} and {Educator} {Should} {Know} {About} {ChatGPT} - {TalkingPoints}},
	url = {https://talkingpts.org/blog/5-things-every-school-district-and-educator-should-know-about-chatgpt/10766/},
	abstract = {In schools and conferences across the country, everyone seems to be talking about AI (Artificial Intelligence) and ChatGPT. Teachers are using ChatGPT to create lesson plans. Students are using ChatGPT to do their homework. And bloggers are using ChatGPT to draft content (although we promise not this one!). We understand the opportunities these resources can},
	language = {en-US},
	urldate = {2023-04-24},
	month = apr,
	year = {2023},
	file = {Snapshot:/Users/johnsmachine/Zotero/storage/Q79Q4LHQ/10766.html:text/html},
}

@misc{NewApproachMitigating,
	title = {A {New} {Approach} {To} {Mitigating} {AI}’s {Negative} {Impact}},
	url = {https://hai.stanford.edu/news/new-approach-mitigating-ais-negative-impact},
	abstract = {Stanford launches an Ethics and Society Review Board that asks researchers to take an early look at the impact of their work.},
	language = {en},
	urldate = {2023-04-26},
	journal = {Stanford HAI},
	file = {Snapshot:/Users/johnsmachine/Zotero/storage/B8NY2WYJ/new-approach-mitigating-ais-negative-impact.html:text/html},
}

@inproceedings{agrawalStanfordMOOCPostsData,
	title = {The {Stanford} {MOOCPosts} {Data} {Set}},
	url = {https://datastage.stanford.edu/StanfordMoocPosts/#appendix1},
	urldate = {2023-04-26},
	publisher = {Stanford},
	author = {Agrawal, Akshay and Paepcke, Andreas},
}

@inproceedings{švábenskýGeneralizableDetectionUrgency2023,
	series = {{EDM} 2023},
	title = {Towards {Generalizable} {Detection} of {Urgency} of {Discussion} {Forum} {Posts}},
	url = {https://github.com/pcla-code/forum-posts-urgency},
	booktitle = {Proceedings of the 16th {International} {Conference} on {Educational} {Data} {Mining}},
	author = {Švábenský, Valdemar and Baker, Ryan S. and Zambrano, Andrés and Zou, Yishan and Slater, Stefan},
	year = {2023},
}

@misc{singhMeetRedPajamaAI2023,
	title = {Meet {RedPajama}: {An} {AI} {Project} to {Create} {Fully} {Open}-{Source} {Large} {Language} {Models} {Beginning} with the {Release} of a 1.2 {Trillion} {Token} {Dataset}},
	shorttitle = {Meet {RedPajama}},
	url = {https://www.marktechpost.com/2023/04/21/meet-redpajama-an-ai-project-to-create-fully-open-source-large-language-models-beginning-with-the-release-of-a-1-2-trillion-token-dataset/},
	abstract = {The most advanced foundation models for AI are only partially open-source and are only available through commercial APIs. This restricts their use and limits research and customization. However, a project called RedPajama now aims to create leading, fully open-source models. The first step of this project, reproducing the LLaMA training dataset, has been completed. Open-source models have made significant progress recently, and AI is experiencing a moment similar to the Linux movement. Stable Diffusion demonstrated that open-source models could compete with commercial offerings and encourage creativity through community participation. A similar movement has now emerged around large language models, with},
	language = {en-US},
	urldate = {2023-04-26},
	journal = {MarkTechPost},
	author = {Singh, Niharika},
	month = apr,
	year = {2023},
	file = {Snapshot:/Users/johnsmachine/Zotero/storage/UPANHLRB/meet-redpajama-an-ai-project-to-create-fully-open-source-large-language-models-beginning-with-t.html:text/html},
}

@misc{OfficeOverachieversWon2023,
	title = {Office {Overachievers} {Won}'t {Be} {Happy} {About} {ChatGPT}, {Study} {Finds}},
	url = {https://gizmodo.com/ai-chatgpt-customer-service-overachievers-productive-1850368672},
	abstract = {Customer support agents given access to a generative AI chatbot were 14\% more productive, but those gains were much higher for lower-performing workers.},
	language = {en},
	urldate = {2023-04-26},
	journal = {Gizmodo},
	month = apr,
	year = {2023},
	file = {Snapshot:/Users/johnsmachine/Zotero/storage/8SG8II3M/ai-chatgpt-customer-service-overachievers-productive-1850368672.html:text/html},
}

@misc{rajiOutsiderOversightDesigning2022,
	title = {Outsider {Oversight}: {Designing} a {Third} {Party} {Audit} {Ecosystem} for {AI} {Governance}},
	shorttitle = {Outsider {Oversight}},
	url = {http://arxiv.org/abs/2206.04737},
	doi = {10.48550/arXiv.2206.04737},
	abstract = {Much attention has focused on algorithmic audits and impact assessments to hold developers and users of algorithmic systems accountable. But existing algorithmic accountability policy approaches have neglected the lessons from non-algorithmic domains: notably, the importance of interventions that allow for the effective participation of third parties. Our paper synthesizes lessons from other fields on how to craft effective systems of external oversight for algorithmic deployments. First, we discuss the challenges of third party oversight in the current AI landscape. Second, we survey audit systems across domains - e.g., financial, environmental, and health regulation - and show that the institutional design of such audits are far from monolithic. Finally, we survey the evidence base around these design components and spell out the implications for algorithmic auditing. We conclude that the turn toward audits alone is unlikely to achieve actual algorithmic accountability, and sustained focus on institutional design will be required for meaningful third party involvement.},
	urldate = {2023-04-26},
	publisher = {arXiv},
	author = {Raji, Inioluwa Deborah and Xu, Peggy and Honigsberg, Colleen and Ho, Daniel E.},
	month = jun,
	year = {2022},
	note = {arXiv:2206.04737 [cs]},
	keywords = {Computer Science - Computers and Society},
	file = {arXiv Fulltext PDF:/Users/johnsmachine/Zotero/storage/2RRI8LP3/Raji et al. - 2022 - Outsider Oversight Designing a Third Party Audit .pdf:application/pdf;arXiv.org Snapshot:/Users/johnsmachine/Zotero/storage/3C4PSMSW/2206.html:text/html},
}

@misc{bernsteinESREthicsSociety2021,
	title = {{ESR}: {Ethics} and {Society} {Review} of {Artificial} {Intelligence} {Research}},
	shorttitle = {{ESR}},
	url = {http://arxiv.org/abs/2106.11521},
	doi = {10.48550/arXiv.2106.11521},
	abstract = {Artificial intelligence (AI) research is routinely criticized for its real and potential impacts on society, and we lack adequate institutional responses to this criticism and to the responsibility that it reflects. AI research often falls outside the purview of existing feedback mechanisms such as the Institutional Review Board (IRB), which are designed to evaluate harms to human subjects rather than harms to human society. In response, we have developed the Ethics and Society Review board (ESR), a feedback panel that works with researchers to mitigate negative ethical and societal aspects of AI research. The ESR's main insight is to serve as a requirement for funding: researchers cannot receive grant funding from a major AI funding program at our university until the researchers complete the ESR process for the proposal. In this article, we describe the ESR as we have designed and run it over its first year across 41 proposals. We analyze aggregate ESR feedback on these proposals, finding that the panel most commonly identifies issues of harms to minority groups, inclusion of diverse stakeholders in the research plan, dual use, and representation in data. Surveys and interviews of researchers who interacted with the ESR found that 58\% felt that it had influenced the design of their research project, 100\% are willing to continue submitting future projects to the ESR, and that they sought additional scaffolding for reasoning through ethics and society issues.},
	urldate = {2023-04-26},
	publisher = {arXiv},
	author = {Bernstein, Michael S. and Levi, Margaret and Magnus, David and Rajala, Betsy and Satz, Debra and Waeiss, Charla},
	month = jul,
	year = {2021},
	note = {arXiv:2106.11521 [cs]},
	keywords = {Computer Science - Computers and Society, ethics, K.4},
	file = {arXiv Fulltext PDF:/Users/johnsmachine/Zotero/storage/MF4FZ9NT/Bernstein et al. - 2021 - ESR Ethics and Society Review of Artificial Intel.pdf:application/pdf;arXiv.org Snapshot:/Users/johnsmachine/Zotero/storage/HQJVZZYW/2106.html:text/html},
}

@misc{MathGPTIncomingHow,
	title = {{MathGPT} is incoming; how it fits into undergraduate math education - {The} {Arizona} {State} {Press}},
	url = {https://www.statepress.com/article/2023/04/mathgpt-math-education},
	abstract = {Arizona State University's independent student-run news organization covering Tempe, Phoenix, Mesa and Glendale.},
	urldate = {2023-04-30},
}

@misc{ResponsibleUseGenerative,
	title = {Responsible {Use} of {Generative} {AI} {\textbar} {Deloitte} {US}},
	url = {https://www2.deloitte.com/us/en/pages/consulting/articles/responsible-use-of-generative-ai.html?id=us:2sm:3li:4dcom_share:5awa:6dcom:consulting},
	urldate = {2023-04-28},
}

@misc{YannLeCunLinkedIn,
	title = {Yann {LeCun} on {LinkedIn}: {A} survey of {LLMs} with a practical guide and evolutionary tree. {Number} of… {\textbar} 21 comments},
	shorttitle = {Yann {LeCun} on {LinkedIn}},
	url = {https://www.linkedin.com/posts/yann-lecun_a-survey-of-llms-with-a-practical-guide-and-activity-7057527966540386304-M4_2},
	abstract = {A survey of LLMs with a practical guide and evolutionary tree.

Number of LLMs from Meta \&\#61; 7
Number of open source LLMs from Meta \&\#61; 7

The architecture… {\textbar} 21 comments on LinkedIn},
	urldate = {2023-04-28},
}

@misc{StudyFindsChatGPT,
	title = {Study {Finds} {ChatGPT} {Outperforms} {Physicians} in {High}-{Quality}, {Empathetic} {Answers} to {Patient} {Questions}},
	url = {https://today.ucsd.edu/story/study-finds-chatgpt-outperforms-physicians-in-high-quality-empathetic-answers-to-patient-questions},
	abstract = {A new study published in JAMA Internal Medicine compared written responses from physicians with those from ChatGPT to real-world health questions. A panel of licensed healthcare professionals preferred ChatGPT’s responses 79\% of the time.},
	urldate = {2023-05-01},
}

@misc{deshpandeToxicityChatGPTAnalyzing2023,
	title = {Toxicity in {ChatGPT}: {Analyzing} {Persona}-assigned {Language} {Models}},
	shorttitle = {Toxicity in {ChatGPT}},
	url = {http://arxiv.org/abs/2304.05335},
	doi = {10.48550/arXiv.2304.05335},
	abstract = {Large language models (LLMs) have shown incredible capabilities and transcended the natural language processing (NLP) community, with adoption throughout many services like healthcare, therapy, education, and customer service. Since users include people with critical information needs like students or patients engaging with chatbots, the safety of these systems is of prime importance. Therefore, a clear understanding of the capabilities and limitations of LLMs is necessary. To this end, we systematically evaluate toxicity in over half a million generations of ChatGPT, a popular dialogue-based LLM. We find that setting the system parameter of ChatGPT by assigning it a persona, say that of the boxer Muhammad Ali, significantly increases the toxicity of generations. Depending on the persona assigned to ChatGPT, its toxicity can increase up to 6x, with outputs engaging in incorrect stereotypes, harmful dialogue, and hurtful opinions. This may be potentially defamatory to the persona and harmful to an unsuspecting user. Furthermore, we find concerning patterns where specific entities (e.g., certain races) are targeted more than others (3x more) irrespective of the assigned persona, that reflect inherent discriminatory biases in the model. We hope that our findings inspire the broader AI community to rethink the efficacy of current safety guardrails and develop better techniques that lead to robust, safe, and trustworthy AI systems.},
	urldate = {2023-05-01},
	publisher = {arXiv},
	author = {Deshpande, Ameet and Murahari, Vishvak and Rajpurohit, Tanmay and Kalyan, Ashwin and Narasimhan, Karthik},
	month = apr,
	year = {2023},
	note = {arXiv:2304.05335 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{rubyHowChatGPTWorks2023,
	title = {How {ChatGPT} {Works}: {The} {Models} {Behind} {The} {Bot}},
	shorttitle = {How {ChatGPT} {Works}},
	url = {https://towardsdatascience.com/how-chatgpt-works-the-models-behind-the-bot-1ce5fca96286},
	abstract = {A brief introduction to the intuition and methodology behind the chat bot you can’t stop hearing about.},
	language = {en},
	urldate = {2023-05-01},
	journal = {Medium},
	author = {Ruby, Molly},
	month = feb,
	year = {2023},
}

@misc{deshpandeToxicityChatGPTAnalyzing2023a,
	title = {Toxicity in {ChatGPT}: {Analyzing} {Persona}-assigned {Language} {Models}},
	shorttitle = {Toxicity in {ChatGPT}},
	url = {http://arxiv.org/abs/2304.05335},
	doi = {10.48550/arXiv.2304.05335},
	abstract = {Large language models (LLMs) have shown incredible capabilities and transcended the natural language processing (NLP) community, with adoption throughout many services like healthcare, therapy, education, and customer service. Since users include people with critical information needs like students or patients engaging with chatbots, the safety of these systems is of prime importance. Therefore, a clear understanding of the capabilities and limitations of LLMs is necessary. To this end, we systematically evaluate toxicity in over half a million generations of ChatGPT, a popular dialogue-based LLM. We find that setting the system parameter of ChatGPT by assigning it a persona, say that of the boxer Muhammad Ali, significantly increases the toxicity of generations. Depending on the persona assigned to ChatGPT, its toxicity can increase up to 6x, with outputs engaging in incorrect stereotypes, harmful dialogue, and hurtful opinions. This may be potentially defamatory to the persona and harmful to an unsuspecting user. Furthermore, we find concerning patterns where specific entities (e.g., certain races) are targeted more than others (3x more) irrespective of the assigned persona, that reflect inherent discriminatory biases in the model. We hope that our findings inspire the broader AI community to rethink the efficacy of current safety guardrails and develop better techniques that lead to robust, safe, and trustworthy AI systems.},
	urldate = {2023-05-01},
	publisher = {arXiv},
	author = {Deshpande, Ameet and Murahari, Vishvak and Rajpurohit, Tanmay and Kalyan, Ashwin and Narasimhan, Karthik},
	month = apr,
	year = {2023},
	note = {arXiv:2304.05335 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{laytonChatGPTShowMe2023,
	title = {{ChatGPT} — {Show} me the {Data} {Sources}},
	url = {https://medium.com/@dlaytonj2/chatgpt-show-me-the-data-sources-11e9433d57e8},
	abstract = {What are the data sources for ChatGPT ? We are all struggling with the heuristic nature of AI, but knowing where the data comes from might…},
	language = {en},
	urldate = {2023-05-01},
	journal = {Medium},
	author = {Layton, Dennis},
	month = jan,
	year = {2023},
}

@misc{ouyangTrainingLanguageModels2022,
	title = {Training language models to follow instructions with human feedback},
	url = {http://arxiv.org/abs/2203.02155},
	doi = {10.48550/arXiv.2203.02155},
	abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
	urldate = {2023-05-01},
	publisher = {arXiv},
	author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
	month = mar,
	year = {2022},
	note = {arXiv:2203.02155 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/johnsmachine/Zotero/storage/FGUYC68D/Ouyang et al. - 2022 - Training language models to follow instructions wi.pdf:application/pdf;arXiv.org Snapshot:/Users/johnsmachine/Zotero/storage/TG479FKW/2203.html:text/html},
}

@misc{openaiGPT4TechnicalReport2023,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	doi = {10.48550/arXiv.2303.08774},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	urldate = {2023-05-01},
	publisher = {arXiv},
	author = {OpenAI},
	month = mar,
	year = {2023},
	note = {arXiv:2303.08774 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/johnsmachine/Zotero/storage/KNVC6IZF/OpenAI - 2023 - GPT-4 Technical Report.pdf:application/pdf;arXiv.org Snapshot:/Users/johnsmachine/Zotero/storage/JE45YKLC/2303.html:text/html},
}

@misc{march24thWritingSynthHypothesis,
	title = {The {Writing} {Synth} {Hypothesis}},
	url = {https://simon.buckinghamshum.net/2023/03/the-writing-synth-hypothesis/},
	urldate = {2023-05-05},
	author = {March 24th, sbs on and 2023},
}

@misc{AssessmentRedesignGenerative,
	title = {Assessment redesign for generative {AI}: {A} taxonomy of options and their viability},
	shorttitle = {Assessment redesign for generative {AI}},
	url = {https://www.linkedin.com/pulse/assessment-redesign-generative-ai-taxonomy-options-viability-lodge},
	abstract = {with Sarah Howard and Jaclyn Broadbent Since the seemingly sudden emergence of ChatGPT at the end of 2022, there has been significant debate surrounding the impact of text-based generative AI in education. Many jurisdictions initially attempted to ban access to these tools, citing concerns that stud},
	urldate = {2023-05-02},
}

@misc{HowWellLarge,
	title = {How {Well} {Do} {Large} {Language} {Models} {Support} {Clinician} {Information} {Needs}?},
	url = {https://hai.stanford.edu/news/how-well-do-large-language-models-support-clinician-information-needs},
	abstract = {Stanford experts examine the safety and accuracy of GPT-4 in serving curbside consultation needs of doctors.},
	language = {en},
	urldate = {2023-05-19},
	journal = {Stanford HAI},
	file = {Snapshot:/Users/johnsmachine/Zotero/storage/3SPSYFSC/how-well-do-large-language-models-support-clinician-information-needs.html:text/html},
}

@misc{santurkarWhoseOpinionsLanguage2023,
	title = {Whose {Opinions} {Do} {Language} {Models} {Reflect}?},
	url = {http://arxiv.org/abs/2303.17548},
	doi = {10.48550/arXiv.2303.17548},
	abstract = {Language models (LMs) are increasingly being used in open-ended contexts, where the opinions reflected by LMs in response to subjective queries can have a profound impact, both on user satisfaction, as well as shaping the views of society at large. In this work, we put forth a quantitative framework to investigate the opinions reflected by LMs -- by leveraging high-quality public opinion polls and their associated human responses. Using this framework, we create OpinionsQA, a new dataset for evaluating the alignment of LM opinions with those of 60 US demographic groups over topics ranging from abortion to automation. Across topics, we find substantial misalignment between the views reflected by current LMs and those of US demographic groups: on par with the Democrat-Republican divide on climate change. Notably, this misalignment persists even after explicitly steering the LMs towards particular demographic groups. Our analysis not only confirms prior observations about the left-leaning tendencies of some human feedback-tuned LMs, but also surfaces groups whose opinions are poorly reflected by current LMs (e.g., 65+ and widowed individuals). Our code and data are available at https://github.com/tatsu-lab/opinions\_qa.},
	urldate = {2023-05-19},
	publisher = {arXiv},
	author = {Santurkar, Shibani and Durmus, Esin and Ladhak, Faisal and Lee, Cinoo and Liang, Percy and Hashimoto, Tatsunori},
	month = mar,
	year = {2023},
	note = {arXiv:2303.17548 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{santurkarWhoseOpinionsLanguage2023a,
	title = {Whose {Opinions} {Do} {Language} {Models} {Reflect}?},
	url = {http://arxiv.org/abs/2303.17548},
	doi = {10.48550/arXiv.2303.17548},
	abstract = {Language models (LMs) are increasingly being used in open-ended contexts, where the opinions reflected by LMs in response to subjective queries can have a profound impact, both on user satisfaction, as well as shaping the views of society at large. In this work, we put forth a quantitative framework to investigate the opinions reflected by LMs -- by leveraging high-quality public opinion polls and their associated human responses. Using this framework, we create OpinionsQA, a new dataset for evaluating the alignment of LM opinions with those of 60 US demographic groups over topics ranging from abortion to automation. Across topics, we find substantial misalignment between the views reflected by current LMs and those of US demographic groups: on par with the Democrat-Republican divide on climate change. Notably, this misalignment persists even after explicitly steering the LMs towards particular demographic groups. Our analysis not only confirms prior observations about the left-leaning tendencies of some human feedback-tuned LMs, but also surfaces groups whose opinions are poorly reflected by current LMs (e.g., 65+ and widowed individuals). Our code and data are available at https://github.com/tatsu-lab/opinions\_qa.},
	urldate = {2023-05-19},
	publisher = {arXiv},
	author = {Santurkar, Shibani and Durmus, Esin and Ladhak, Faisal and Lee, Cinoo and Liang, Percy and Hashimoto, Tatsunori},
	month = mar,
	year = {2023},
	note = {arXiv:2303.17548 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{LargeCreativeAI,
	title = {Large, creative {AI} models will transform lives and labour markets},
	issn = {0013-0613},
	url = {https://www.economist.com/interactive/science-and-technology/2023/04/22/large-creative-ai-models-will-transform-how-we-live-and-work},
	urldate = {2023-05-21},
	journal = {The Economist},
	file = {The Economist Snapshot:/Users/johnsmachine/Zotero/storage/BRIM3X7L/large-creative-ai-models-will-transform-how-we-live-and-work.html:text/html},
}

@misc{demzskyTeacherCoachingACL,
	title = {Teacher {Coaching} - {ACL} preprint},
	author = {demzsky},
	file = {Wang_Demszky_ChatGPT_Teacher_Coach_2023.pdf:/Users/johnsmachine/Zotero/storage/FXEV8YVE/Wang_Demszky_ChatGPT_Teacher_Coach_2023.pdf:application/pdf},
}

@misc{pardosLearningGainDifferences2023,
	title = {Learning gain differences between {ChatGPT} and human tutor generated algebra hints},
	url = {http://arxiv.org/abs/2302.06871},
	doi = {10.48550/arXiv.2302.06871},
	abstract = {Large Language Models (LLMs), such as ChatGPT, are quickly advancing AI to the frontiers of practical consumer use and leading industries to re-evaluate how they allocate resources for content production. Authoring of open educational resources and hint content within adaptive tutoring systems is labor intensive. Should LLMs like ChatGPT produce educational content on par with human-authored content, the implications would be significant for further scaling of computer tutoring system approaches. In this paper, we conduct the first learning gain evaluation of ChatGPT by comparing the efficacy of its hints with hints authored by human tutors with 77 participants across two algebra topic areas, Elementary Algebra and Intermediate Algebra. We find that 70\% of hints produced by ChatGPT passed our manual quality checks and that both human and ChatGPT conditions produced positive learning gains. However, gains were only statistically significant for human tutor created hints. Learning gains from human-created hints were substantially and statistically significantly higher than ChatGPT hints in both topic areas, though ChatGPT participants in the Intermediate Algebra experiment were near ceiling and not even with the control at pre-test. We discuss the limitations of our study and suggest several future directions for the field. Problem and hint content used in the experiment is provided for replicability.},
	urldate = {2023-05-23},
	publisher = {arXiv},
	author = {Pardos, Zachary A. and Bhandari, Shreya},
	month = feb,
	year = {2023},
	note = {arXiv:2302.06871 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:/Users/johnsmachine/Zotero/storage/DSGAPIPF/Pardos and Bhandari - 2023 - Learning gain differences between ChatGPT and huma.pdf:application/pdf;arXiv.org Snapshot:/Users/johnsmachine/Zotero/storage/8HCBGMEN/2302.html:text/html},
}

@misc{daiCanLargeLanguage2023,
	title = {Can {Large} {Language} {Models} {Provide} {Feedback} to {Students}? {A} {Case} {Study} on {ChatGPT}},
	shorttitle = {Can {Large} {Language} {Models} {Provide} {Feedback} to {Students}?},
	url = {https://edarxiv.org/hcgzj/},
	doi = {10.35542/osf.io/hcgzj},
	abstract = {Educational feedback has been widely acknowledged as an effective approach to improving student learning. However, scaling effective practices can be laborious and costly, which motivated researchers to work on automated feedback systems (AFS). Inspired by the recent advancements in the pre-trained language models (e.g., ChatGPT), we posit that such models might advance the existing knowledge of textual feedback generation in AFS because of their capability to offer natural-sounding and detailed responses. Therefore, we aimed to investigate the feasibility of using ChatGPT to provide students with feedback to help them learn better. Specifically, we first examined the readability of ChatGPT-generated feedback. Then, we measured the agreement between ChatGPT and the instructor when assessing students' assignments according to the marking rubric. Finally, we used a well-known theoretical feedback framework to further investigate the effectiveness of the feedback generated by ChatGPT. Our results show that i) ChatGPT is capable of generating more detailed feedback that fluently and coherently summarizes students' performance than human instructors; ii) ChatGPT achieved high agreement with the instructor when assessing the topic of students' assignments; and iii) ChatGPT could provide feedback on the process of students completing the task, which benefits students developing learning skills.},
	language = {en-us},
	urldate = {2023-05-23},
	publisher = {EdArXiv},
	author = {Dai, Wei and Lin, Jionghao and Jin, Flora and Li, Tongguang and Tsai, Yi-Shan and Gasevic, Dragan and Chen, Guanliang},
	month = apr,
	year = {2023},
	keywords = {and Research, Automated Feedback, Education, Educational Assessment, Educational Methods, Evaluation, Feedback Effectiveness, Feedback Generation, Higher Education, Large Language Model},
	file = {Full Text PDF:/Users/johnsmachine/Zotero/storage/9CHDMF4S/Dai et al. - 2023 - Can Large Language Models Provide Feedback to Stud.pdf:application/pdf},
}

@misc{harris2023,
	title = {FATE (Fairness and Transparency) Hits the Mainstream: UK's 5 Principles for AI Regulation},
	author = {Harris, Robert},
	year = {2023},
	month = {03},
	date = {2023-03-29},
	url = {https://feedzai.com/blog/fate-fairness-and-transparency-hits-the-mainstream-uks-5-principles-for-ai-regulation/},
	langid = {canadian}
}

@article{dorsey2022,
	title = {Validity Arguments Meet Artificial Intelligence in Innovative Educational Assessment: A Discussion and Look Forward},
	author = {Dorsey, David W. and Michaels, Hillary R.},
	year = {2022},
	date = {2022},
	journal = {Journal of Educational Measurement},
	pages = {389--394},
	volume = {59},
	number = {3},
	doi = {10.1111/jedm.12330},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jedm.12330},
	note = {{\_}eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/jedm.12330},
	langid = {en}
}
