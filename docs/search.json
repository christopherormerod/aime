[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The recent explosion of “Generative AI” has broad and deep implications for Education Measurement. By “Generative AI” we mean algorithms that create new text, images, or sound using machine learning algorithms trained on previous existing samples of that data. This technology is already in operational use in assessments and learning solutions and is being rapidly developed.  At the same time, it brings significant challenges to foundational principles in measurement validity, fairness, and reliability. With evidence and constructs constantly recreated, and data not representative nor static, many concepts of measurement and practices to implement them need to be rethought.\nGrounding innovation with principles from measurement can help to reduce the risk of these new approaches, improve their impact on student learning, and ensure that students from minoritized communities are not harmed, without rejecting their potential value. This genie will not be put back into its bottle.  Instead, measurement experts can help shape these solutions, collaboratively identifying important areas of focus and sharing innovative solutions. \nThis special interest group (SIGIME) seeks to advance the theoretical and applied research into Generative AI of educational measurement by bringing together data scientists, psychometricians, education researchers, and other interested stakeholders. The SIGIME will discuss current practices in using Generative AI, approaches to evaluate their precision/accuracy, and areas where more foundational research is required into the way we test and measure educational outcomes. This group seeks to create a strong professional identity and intellectual home for those interested in the use of AI in many areas, including automated scoring, item evaluation, validity studies, formative feedback, and generative AI for automated item generation. A critical initial responsibility is to ensure that there are guidelines around FATE (fair, accountable, transparent, and ethical) principles for applying AI in measurement (Harris 2023 ). Further, the interpretability of machine learning models is often a difficulty in evaluating the validity of these methods using conventional psychometric approaches, although there is active research in this area (Dorsey and Michaels 2022).\nWe suggest three initial applied areas for exploration: \n\nItem Generation. This new technology is rapidly changing the way we think about education and problem-solving. With generative AI, the ability to create an infinite item pool is within our grasp and one that is customized to the interests or social context of the learner. However, some important ethical and psychometric considerations need to be addressed. How do we filter this pool for the best items? What are our standards of validity and accuracy when there is no baseline? How should assessment item development processes be designed? It is important for those of us who understand these systems best to help educators and test developers make informed decisions about how to use generative AI fairly and appropriately.\nAutomated scoring of open-ended items. Automated Scoringis possibly the most widely used application of AI in education, and has been identified as the top use of AI published in measurement journals (Zheng et. all, 2023). There has been a major shift in the past few years in the methods used. The development and incorporation of large language models (LLMs) in automated scoring systems have led to substantial increases in scoring accuracy and increased flexibility to generalize models to new items with a lower training effort. However, LLMs are difficult to interpret and have a greater risk of bias. Since LLMs are trained on large amounts of data that is not from students (fortunately), this introduces new ways in which scoring engines could be biased. As we move forward, we need to ensure that using LLMs in automated scoring remains fair and equitable.\nFormative Feedback. In many contexts, assessment is rapidly moving from an isolated activity to one that is embedded within the learning experience. As AI methods continue to advance, an open area of research as to how we can leverage AI to provide students with improved diagnostic feedback. Integrating AI-based tools into automated writing evaluation (AWE) systems requires that the feedback returned is grounded in educational theory. Dialogue between data scientists, educators, and psychometricians is key to ensuring that AI is utilized to improve educational outcomes. A special interest group within the NCME could provide an appropriate forum for these discussions. \n\nGenerative AI is taking the world by storm, and we believe NCME can make a deep and substantive contribution to how this technology is used in education.  A special interest group on Artificial Intelligence (AI) in Measurement and Education would provide a timely forum for researchers and practitioners in the use of AI in Education and Measurement to share their work, discuss challenges, and develop best practices. \n\n\n\n\n\n\nReferences\n\nDorsey, David W., and Hillary R. Michaels. 2022. “Validity Arguments Meet Artificial Intelligence in Innovative Educational Assessment: A Discussion and Look Forward.” Journal of Educational Measurement 59 (3): 389–94. https://doi.org/10.1111/jedm.12330.\n\n\nHarris, Robert. 2023. “FATE (Fairness and Transparency) Hits the Mainstream: UK’s 5 Principles for AI Regulation.” https://feedzai.com/blog/fate-fairness-and-transparency-hits-the-mainstream-uks-5-principles-for-ai-regulation/."
  },
  {
    "objectID": "events.html",
    "href": "events.html",
    "title": "Events",
    "section": "",
    "text": "On this page, we aim to keep you up-to-date on events relevant to AI in Education.\n\nMember Events\nWe will be holding a first meeting\n\nJune 14th 4:00pm - 5:00pm (EST) over Zoom.\n\n\n\nConferences\n\nAssociation of Test Publisher’s Fourth EdTech and Computational Psychometrics Summit: The 2023 ATP ECPS will be a two-day virtual summit taking place on June 6th and June 7th, 2023, with an optional Pre-Conference Workshop on June 5th, focusing on how EdTech & computational psychometrics’ business of assessment and innovation can help conceive digital assessments for lifelong learning. (June 5-7, 2023)\nThe 24th International Conference on Artificial Intelligence in Education: The 24th International Conference on Artificial Intelligence in Education, AIED 2023, will take place July 3-7, 2023 in Tokyo, Japan and virtually. (3-7, July 2023)\n18th Workshop on Innovative Use of NLP for Building Educational Applications: The BEA Workshop is a leading venue for NLP innovation in the context of educational applications. (July 13, 2023)\nThe 17th International Workshop on Semantic Evaluation: SemEval is a series of international natural language processing (NLP) research workshops whose mission is to advance the current state of the art in semantic analysis and to help create high-quality annotated datasets in a range of increasingly challenging problems in natural language semantics. (July 13-14 2023).\nThe First Conference on AI Generated Content 2023: As the use of AI continues to expand and evolve, the potential for its impact on content creation is becoming increasingly apparent. Recently, ChatGPT has made a significant impact and has helped to advance the development on the field of AI-generated content. AI-generated content can refer to a range of techniques and applications that use machine learning and deep learning models to automatically generate text, speech, or other forms of content that mimic human-generated content.\n\nTo register an event, please use the following form: GAIME Event Registration"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Welcome to the (proposed) Special Interest Group on Generative Artificial Intelligence in Measurement and Education (GAIME) with the National Council of Measurement in Education. Our goal is to serve as an intellectual home for data scientists and psychometricians interested in the application of AI to measurement and education.\nA draft of our charter can be found here.\nInterested in joining us? The signup form can be found here. We currently hold monthly meetings and are organizing a speaker series."
  },
  {
    "objectID": "literature.html",
    "href": "literature.html",
    "title": "Literature",
    "section": "",
    "text": "This is a collection of references to empirical studies, conceptual articles about LLMs in education, and other publications relevant to understanding the current moment. This is a snapshot (updated 5/25/23) of a Zotero group; read/write access available on request if you have a suggestion or would like to see the latest & greatest.\nThis is a work in progress; a current formatted version of the literature is available here"
  },
  {
    "objectID": "open-datasets.html",
    "href": "open-datasets.html",
    "title": "Open Datasets",
    "section": "",
    "text": "Training data is required for ML; often authentic data can be difficult to come by. This is a collection of datasets that are generally useful.\nIf you have a dataset you’d like to share (or want to find one not listed here), you might consider one of the open data repositories (e.g. Google Research Datasets, EdData, Data.gov, Dataverse, ICPSR, Datashop, UCI ML Repository).\n\nAutomated Essay Scoring\n\nKaggle ASAP AES Dataset: A dataset containing essays to 8 different essay prompts.\n\n\n\nAutomated Short Answer Scoring\n\nKaggle ASAP SAS Dataset: A dataset containing student responses to 10 different short constructed response prompts.\nPowergrading Short Answer Grading Corpus: This corpus contains the original data analyzed in the following paper: Basu, Jacobs, and Vanderwende, “Powergrading: a Clustering Approach to Amplify Human Effort for Short Answer Grading,” Transactions of the ACL, 2013. Last published: October 4, 2013.\n\n\n\nFormative Feedback\n\nPERSUADE Corpus: The Persuasive Essays for Rating, Selecting, and Understanding Argumentative and Discourse Elements (PERSUADE) corpus which contains over 280,000 discourse annotations for over 25,000 argumentative essays.\nIteraTeR, R3 System, and DEIIteraTeR: A repository that provides datasets and code for preprocessing, training and testing models for Iterative Text Revision.\n\n\n\nItem Generation\nTBA\n\n\nProcess Data for Learning Pathways\n\nEdNet - dataset of all student-system interactions collected over 2 years by Santa, a multi-platform AI tutoring service with more than 780K users in Korea available through Android, iOS and web. Details of dataset are available as a pre–print.\nHarvardX-MITx Person-Course Dataset- this dataset contains information from the first year of edX courses, which are Harvard and MIT’s Massive Open Online Courses (MOOCs). The data includes over 600,000 students and over a billion records.\nOpen University Learning Analytics dataset - contains data about courses, students and their interactions with Virtual Learning Environment (VLE) for seven selected courses (called modules). Presentations of courses start in February and October - they are marked by “B” and “J” respectively. The dataset consists of tables connected using unique identifiers. All tables are stored in the csv format.\n\n\n\nUnlabelled\n\nEvaluating Student Writing"
  }
]